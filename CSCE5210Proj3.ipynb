{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8fe3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project 3 - MDP + Value Iteration for Parallel Parking\n",
    "\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Tuple, List, Iterable, Optional\n",
    "\n",
    "# ============================================================\n",
    "# 1. TYPES AND GLOBAL CONFIG\n",
    "# ============================================================\n",
    "\n",
    "State = Tuple[int, int]      # (col, row), 1-based indexing\n",
    "Action = str                 # \"N\", \"NE\", \"E\", \"SE\", \"S\", \"SW\", \"W\", \"NW\"\n",
    "\n",
    "# 8-directional movement\n",
    "ACTION_TO_DELTA: Dict[Action, Tuple[int, int]] = {\n",
    "    \"N\":  (0, 1),\n",
    "    \"NE\": (1, 1),\n",
    "    \"E\":  (1, 0),\n",
    "    \"SE\": (1, -1),\n",
    "    \"S\":  (0, -1),\n",
    "    \"SW\": (-1, -1),\n",
    "    \"W\":  (-1, 0),\n",
    "    \"NW\": (-1, 1),\n",
    "}\n",
    "ALL_ACTIONS: List[Action] = list(ACTION_TO_DELTA.keys())\n",
    "\n",
    "# Hazards: row 1, col 1 and row 5, col 1 → (1,1), (1,5)\n",
    "HAZARD_STATES: Tuple[State, ...] = (\n",
    "    (1, 1),\n",
    "    (1, 5),\n",
    ")\n",
    "\n",
    "# Start states S1..S7 (top row, columns 2–8)\n",
    "START_STATES: List[State] = [\n",
    "    (2, 5),  # S1\n",
    "    (3, 5),  # S2\n",
    "    (4, 5),  # S3\n",
    "    (5, 5),  # S4\n",
    "    (6, 5),  # S5\n",
    "    (7, 5),  # S6\n",
    "    (8, 5),  # S7\n",
    "]\n",
    "\n",
    "# Goal: row 3, col 1 → (1,3)\n",
    "GOAL_STATE: State = (1, 3)\n",
    "\n",
    "# R is 1000: -R for hazards and +R for goal\n",
    "GOAL_REWARD: float = 1000.0\n",
    "HAZARD_REWARD: float = -1000.0\n",
    "\n",
    "# Noise and discount\n",
    "DISCOUNT_FACTOR: float = 0.9\n",
    "NOISE: float = 0.1\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. UTILITY HELPERS\n",
    "# ============================================================\n",
    "\n",
    "def is_opposite(a1: Action, a2: Action) -> bool:\n",
    "    \"\"\"Return True if a2 is the 180° opposite direction of a1.\"\"\"\n",
    "    dc1, dr1 = ACTION_TO_DELTA[a1]\n",
    "    dc2, dr2 = ACTION_TO_DELTA[a2]\n",
    "    return (dc1 + dc2 == 0) and (dr1 + dr2 == 0)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. GRIDWORLD MDP\n",
    "# ============================================================\n",
    "\n",
    "@dataclass\n",
    "class GridWorldMDP:\n",
    "    width: int = 8\n",
    "    height: int = 5\n",
    "    hazard_states: Tuple[State, ...] = HAZARD_STATES\n",
    "    hazard_reward: float = HAZARD_REWARD\n",
    "    goal_state: State = GOAL_STATE\n",
    "    goal_reward: float = GOAL_REWARD\n",
    "    live_reward: float = -1.0          # r in [-20, 0], will be varied\n",
    "    discount: float = DISCOUNT_FACTOR\n",
    "    noise: float = NOISE\n",
    "    obstacle_states: Tuple[State, ...] = ()   # for R2: add (4, 3)\n",
    "\n",
    "    # -------- basic state helpers --------\n",
    "\n",
    "    def get_states(self) -> List[State]:\n",
    "        return [\n",
    "            (c, r)\n",
    "            for c in range(1, self.width + 1)\n",
    "            for r in range(1, self.height + 1)\n",
    "        ]\n",
    "\n",
    "    def in_bounds(self, s: State) -> bool:\n",
    "        c, r = s\n",
    "        return 1 <= c <= self.width and 1 <= r <= self.height\n",
    "\n",
    "    def is_obstacle(self, s: State) -> bool:\n",
    "        return s in self.obstacle_states\n",
    "\n",
    "    def is_terminal(self, s: State) -> bool:\n",
    "        return s == self.goal_state or s in self.hazard_states\n",
    "\n",
    "    def reward(self, s: State) -> float:\n",
    "        if s == self.goal_state:\n",
    "            return self.goal_reward\n",
    "        if s in self.hazard_states:\n",
    "            return self.hazard_reward\n",
    "        # everything else uses live-in reward r\n",
    "        return self.live_reward\n",
    "\n",
    "    # -------- transition model --------\n",
    "\n",
    "    def get_possible_actions(self, s: State) -> Iterable[Action]:\n",
    "        if self.is_terminal(s) or self.is_obstacle(s):\n",
    "            return []\n",
    "        return ALL_ACTIONS\n",
    "\n",
    "    def _is_blocked(self, dest: State) -> bool:\n",
    "        \"\"\"\n",
    "        Blocked if:\n",
    "        - dest is off-grid\n",
    "        - dest is an obstacle\n",
    "        (Opposite direction is handled by never including it as a candidate.)\n",
    "        \"\"\"\n",
    "        if not self.in_bounds(dest):\n",
    "            return True\n",
    "        if self.is_obstacle(dest):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def get_transition_probs(self, s: State, a: Action) -> List[Tuple[State, float]]:\n",
    "        \"\"\"\n",
    "        Implements the TransProb(s, a) logic from Tutorial 5:\n",
    "        - Intended state gets probability 0.9 (1 - noise) if not blocked\n",
    "        - Blocked intended move: stay put (prob 1.0)\n",
    "        - Other non-blocked destinations share noise / (n_dest(s) - 1)\n",
    "        - Opposite direction never taken\n",
    "        \"\"\"\n",
    "        if self.is_terminal(s):\n",
    "            return [(s, 1.0)]\n",
    "\n",
    "        # Intended destination\n",
    "        dc, dr = ACTION_TO_DELTA[a]\n",
    "        intended_dest = (s[0] + dc, s[1] + dr)\n",
    "\n",
    "        # Collect all candidate destinations (non-blocked, excluding opposite)\n",
    "        destinations: List[State] = []\n",
    "        for other_action, (odc, odr) in ACTION_TO_DELTA.items():\n",
    "            if is_opposite(a, other_action):\n",
    "                continue  # never move opposite to intended direction\n",
    "            dest = (s[0] + odc, s[1] + odr)\n",
    "            if not self._is_blocked(dest):\n",
    "                destinations.append(dest)\n",
    "\n",
    "        if self._is_blocked(intended_dest):\n",
    "            # If intended is blocked, agent remains in current state (p=1)\n",
    "            return [(s, 1.0)]\n",
    "\n",
    "        # n_dest(s): number of (non-blocked) destinations\n",
    "        n_dest = len(destinations)\n",
    "        if n_dest == 0:\n",
    "            # no possible moves; stay put\n",
    "            return [(s, 1.0)]\n",
    "\n",
    "        transitions: Dict[State, float] = {}\n",
    "\n",
    "        # Intended move gets 0.9\n",
    "        intended_prob = 1.0 - self.noise\n",
    "        transitions[intended_dest] = transitions.get(intended_dest, 0.0) + intended_prob\n",
    "\n",
    "        # Other moves share noise\n",
    "        other_dests = [d for d in destinations if d != intended_dest]\n",
    "        if len(other_dests) > 0:\n",
    "            per_unintended = self.noise / len(other_dests)\n",
    "            for d in other_dests:\n",
    "                transitions[d] = transitions.get(d, 0.0) + per_unintended\n",
    "        else:\n",
    "            # only intended destination exists\n",
    "            transitions[intended_dest] += self.noise\n",
    "\n",
    "        # Normalize for safety\n",
    "        total = sum(transitions.values())\n",
    "        for st in list(transitions.keys()):\n",
    "            transitions[st] /= total\n",
    "\n",
    "        return list(transitions.items())\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. VALUE ITERATION\n",
    "# ============================================================\n",
    "\n",
    "def value_iteration(\n",
    "    mdp: GridWorldMDP,\n",
    "    theta: float = 1e-3,\n",
    "    max_iterations: int = 500,\n",
    ") -> Tuple[Dict[State, float], Dict[State, Optional[Action]]]:\n",
    "    \"\"\"\n",
    "    Standard Bellman optimality value iteration.\n",
    "    U(s) = max_a Σ_s' P(s'|s,a)[R(s') + γ U(s')]\n",
    "    \"\"\"\n",
    "    states = mdp.get_states()\n",
    "    U: Dict[State, float] = {s: 0.0 for s in states}\n",
    "    policy: Dict[State, Optional[Action]] = {s: None for s in states}\n",
    "\n",
    "    for _ in range(max_iterations):\n",
    "        delta = 0.0\n",
    "        U_new = U.copy()\n",
    "\n",
    "        for s in states:\n",
    "            if mdp.is_terminal(s) or mdp.is_obstacle(s):\n",
    "                U_new[s] = mdp.reward(s)\n",
    "                policy[s] = None\n",
    "                continue\n",
    "\n",
    "            actions = list(mdp.get_possible_actions(s))\n",
    "            if not actions:\n",
    "                U_new[s] = mdp.reward(s)\n",
    "                policy[s] = None\n",
    "                continue\n",
    "\n",
    "            best_value = float(\"-inf\")\n",
    "            best_action: Optional[Action] = None\n",
    "\n",
    "            for a in actions:\n",
    "                expected_util = 0.0\n",
    "                for s_prime, prob in mdp.get_transition_probs(s, a):\n",
    "                    r_sp = mdp.reward(s_prime)\n",
    "                    expected_util += prob * (r_sp + mdp.discount * U[s_prime])\n",
    "\n",
    "                if expected_util > best_value:\n",
    "                    best_value = expected_util\n",
    "                    best_action = a\n",
    "\n",
    "            U_new[s] = best_value\n",
    "            policy[s] = best_action\n",
    "            delta = max(delta, abs(U_new[s] - U[s]))\n",
    "\n",
    "        U = U_new\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    return U, policy\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5. POLICY SCORING FOR P1 (R1)\n",
    "# ============================================================\n",
    "\n",
    "def score_policy_P1(\n",
    "    mdp: GridWorldMDP,\n",
    "    U: Dict[State, float],\n",
    "    policy: Dict[State, Optional[Action]],\n",
    "    max_steps: int = 50,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Approximate the score of P1 as described in the spec:\n",
    "    For each start state in S1..S7, follow the policy greedily and\n",
    "    sum the utilities along the path until reaching a terminal state\n",
    "    or max_steps.\n",
    "    \"\"\"\n",
    "    total_score = 0.0\n",
    "\n",
    "    for s0 in START_STATES:\n",
    "        s = s0\n",
    "        path_value = 0.0\n",
    "\n",
    "        for _ in range(max_steps):\n",
    "            path_value += U[s]\n",
    "            if mdp.is_terminal(s):\n",
    "                break\n",
    "\n",
    "            a = policy.get(s)\n",
    "            if a is None:\n",
    "                break\n",
    "\n",
    "            transitions = mdp.get_transition_probs(s, a)\n",
    "            # follow the most probable next state\n",
    "            s = max(transitions, key=lambda t: t[1])[0]\n",
    "\n",
    "        total_score += path_value\n",
    "\n",
    "    return total_score\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6. VISUALIZATION HELPERS\n",
    "# ============================================================\n",
    "\n",
    "def print_utility_grid(mdp: GridWorldMDP, U: Dict[State, float]) -> None:\n",
    "    \"\"\"\n",
    "    Print utilities in grid form (row 5 down to 1, col 1 to 8).\n",
    "    \"\"\"\n",
    "    for r in range(mdp.height, 0, -1):\n",
    "        row_vals = []\n",
    "        for c in range(1, mdp.width + 1):\n",
    "            row_vals.append(f\"{U[(c, r)]:7.1f}\")\n",
    "        print(\" \".join(row_vals))\n",
    "    print()\n",
    "\n",
    "\n",
    "def print_policy_grid(mdp: GridWorldMDP, policy: Dict[State, Optional[Action]]) -> None:\n",
    "    \"\"\"\n",
    "    Print policy arrows / labels for each cell.\n",
    "    \"\"\"\n",
    "    for r in range(mdp.height, 0, -1):\n",
    "        row_vals = []\n",
    "        for c in range(1, mdp.width + 1):\n",
    "            s = (c, r)\n",
    "            if mdp.is_terminal(s):\n",
    "                if s == mdp.goal_state:\n",
    "                    row_vals.append(\" G \")\n",
    "                else:\n",
    "                    row_vals.append(\" H \")\n",
    "            elif mdp.is_obstacle(s):\n",
    "                row_vals.append(\" X \")\n",
    "            else:\n",
    "                a = policy.get(s)\n",
    "                row_vals.append(f\"{a:>3}\" if a is not None else \" . \")\n",
    "        print(\" \".join(row_vals))\n",
    "    print()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7. R1: SEARCH OVER r IN [-20, 0]\n",
    "# ============================================================\n",
    "\n",
    "def find_best_r_for_original_env(\n",
    "    r_min: int = -20,\n",
    "    r_max: int = 0,\n",
    ") -> Tuple[int, Dict[State, float], Dict[State, Optional[Action]]]:\n",
    "    \"\"\"\n",
    "    R1: Search r in [r_min, r_max] and find the FIRST r that yields\n",
    "    the best P1 according to the policy score.\n",
    "    \"\"\"\n",
    "    best_score = float(\"-inf\")\n",
    "    best_r: Optional[int] = None\n",
    "    best_U: Optional[Dict[State, float]] = None\n",
    "    best_policy: Optional[Dict[State, Optional[Action]]] = None\n",
    "\n",
    "    for r in range(r_min, r_max + 1):\n",
    "        mdp = GridWorldMDP(live_reward=float(r))\n",
    "        U, policy = value_iteration(mdp)\n",
    "        score = score_policy_P1(mdp, U, policy)\n",
    "\n",
    "        print(f\"r = {r:3d}, policy score = {score:.2f}\")\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_r = r\n",
    "            best_U = U\n",
    "            best_policy = policy\n",
    "\n",
    "    print(\"\\n=== R1 RESULT ===\")\n",
    "    print(f\"Best score across r in [{r_min}, {r_max}]: {best_score:.2f}\")\n",
    "    print(f\"First r achieving this best score: r = {best_r}\")\n",
    "\n",
    "    return best_r, best_U, best_policy\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 8. R2: COMPARE ORIGINAL ENVIRONMENT WITH OBSTACLE AT (4, 3)\n",
    "# ============================================================\n",
    "\n",
    "def compare_original_and_obstacle_env(best_r: int) -> None:\n",
    "    \"\"\"\n",
    "    R2: Add an obstacle at row 3, col 4 → (4,3), recompute optimal P2,\n",
    "    and compare it with P1.\n",
    "    \"\"\"\n",
    "    mdp_original = GridWorldMDP(live_reward=float(best_r))\n",
    "    U1, policy1 = value_iteration(mdp_original)\n",
    "\n",
    "    mdp_obstacle = GridWorldMDP(\n",
    "        live_reward=float(best_r),\n",
    "        obstacle_states=((4, 3),)   # row 3, column 4\n",
    "    )\n",
    "    U2, policy2 = value_iteration(mdp_obstacle)\n",
    "\n",
    "    same = True\n",
    "    for s in mdp_original.get_states():\n",
    "        if policy1.get(s) != policy2.get(s):\n",
    "            same = False\n",
    "            print(f\"Policy differs at state {s}: P1={policy1.get(s)}, P2={policy2.get(s)}\")\n",
    "\n",
    "    if same:\n",
    "        print(\"\\nP2 is identical to P1.\")\n",
    "    else:\n",
    "        print(\"\\nP2 differs from P1 in the states listed above.\")\n",
    "\n",
    "    print(\"\\n--- Original environment policy (P1) ---\")\n",
    "    print_policy_grid(mdp_original, policy1)\n",
    "\n",
    "    print(\"\\n--- New environment policy (P2) with obstacle at (4, 3) ---\")\n",
    "    print_policy_grid(mdp_obstacle, policy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8eb7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r = -20, policy score = 49562.42\n",
      "r = -19, policy score = 49620.64\n",
      "r = -18, policy score = 49678.85\n",
      "r = -17, policy score = 49737.07\n",
      "r = -16, policy score = 49795.28\n",
      "r = -15, policy score = 49853.50\n",
      "r = -14, policy score = 49911.71\n",
      "r = -13, policy score = 49969.92\n",
      "r = -12, policy score = 50028.14\n",
      "r = -11, policy score = 50086.35\n",
      "r = -10, policy score = 50144.57\n",
      "r =  -9, policy score = 50202.78\n",
      "r =  -8, policy score = 50261.00\n",
      "r =  -7, policy score = 50319.21\n",
      "r =  -6, policy score = 50377.42\n",
      "r =  -5, policy score = 50435.64\n",
      "r =  -4, policy score = 50493.85\n",
      "r =  -3, policy score = 50552.07\n",
      "r =  -2, policy score = 50610.28\n",
      "r =  -1, policy score = 50668.50\n",
      "r =   0, policy score = 50726.71\n",
      "\n",
      "=== R1 RESULT ===\n",
      "Best score across r in [-20, 0]: 50726.71\n",
      "First r achieving this best score: r = 0\n",
      "\n",
      "Utility grid for best r:\n",
      "-1000.0  1579.4  1603.7  1472.4  1314.3  1171.0  1043.2   936.1\n",
      " 1867.7  1807.9  1659.0  1479.9  1318.5  1174.2  1045.8   936.3\n",
      " 1000.0  1870.0  1663.1  1480.9  1318.7  1174.3  1045.9   936.3\n",
      " 1867.7  1807.9  1659.0  1479.9  1318.5  1174.2  1045.8   936.3\n",
      "-1000.0  1579.4  1603.7  1472.4  1314.3  1171.0  1043.2   936.1\n",
      "\n",
      "Policy grid for best r:\n",
      " H   SW  SW  SW  SW  SW   W  SW\n",
      "  S  SW  SW  SW  SW  SW  SW  SW\n",
      " G    W   W   W   W   W   W   W\n",
      "  N  NW  NW  NW  NW  NW  NW  NW\n",
      " H   NW  NW  NW  NW  NW   W  NW\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 9. RUN R1\n",
    "# ============================================================\n",
    "\n",
    "best_r, best_U, best_policy = find_best_r_for_original_env()\n",
    "mdp_best = GridWorldMDP(live_reward=float(best_r))\n",
    "print(\"\\nUtility grid for best r:\")\n",
    "print_utility_grid(mdp_best, best_U)\n",
    "print(\"Policy grid for best r:\")\n",
    "print_policy_grid(mdp_best, best_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0db9634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy differs at state (4, 3): P1=W, P2=None\n",
      "Policy differs at state (5, 2): P1=NW, P2=W\n",
      "Policy differs at state (5, 3): P1=W, P2=SW\n",
      "Policy differs at state (5, 4): P1=SW, P2=W\n",
      "Policy differs at state (6, 1): P1=NW, P2=W\n",
      "Policy differs at state (6, 5): P1=SW, P2=W\n",
      "\n",
      "P2 differs from P1 in the states listed above.\n",
      "\n",
      "--- Original environment policy (P1) ---\n",
      " H   SW  SW  SW  SW  SW   W  SW\n",
      "  S  SW  SW  SW  SW  SW  SW  SW\n",
      " G    W   W   W   W   W   W   W\n",
      "  N  NW  NW  NW  NW  NW  NW  NW\n",
      " H   NW  NW  NW  NW  NW   W  NW\n",
      "\n",
      "\n",
      "--- New environment policy (P2) with obstacle at (4, 3) ---\n",
      " H   SW  SW  SW  SW   W   W  SW\n",
      "  S  SW  SW  SW   W  SW  SW  SW\n",
      " G    W   W  X   SW   W   W   W\n",
      "  N  NW  NW  NW   W  NW  NW  NW\n",
      " H   NW  NW  NW  NW   W   W  NW\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 9. RUN R2\n",
    "# ============================================================\n",
    "compare_original_and_obstacle_env(best_r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-tfmetal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
